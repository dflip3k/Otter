import requests
import torch
import transformers
import json
from PIL import Image
# from otter.modeling_otter import OtterForConditionalGeneration
import argparse
from flamingo.modeling_flamingo import FlamingoForConditionalGeneration



def get_formatted_prompt(prompt: str) -> str:
    return f"<image> User: {prompt} GPT: <answer>"


def get_response(path: str, prompt: str) -> str:
    query_image = Image.open(path)
    vision_x = (
        image_processor.preprocess([query_image], return_tensors="pt")["pixel_values"]
        .unsqueeze(1)
        .unsqueeze(0)
    )
    lang_x = model.text_tokenizer(
        [
            get_formatted_prompt(prompt),
        ],
        return_tensors="pt",
    )
    import pdb;pdb.set_trace()
    generated_text = model.generate(
        vision_x=vision_x.to(model.device),
        lang_x=lang_x["input_ids"].to(model.device),
        attention_mask=lang_x["attention_mask"].to(model.device),
        max_new_tokens=256,
        num_beams=3,
        no_repeat_ngram_size=3,
    )
    parsed_output = (
        model.text_tokenizer.decode(generated_text[0])
        .split("<answer>")[1]
        .lstrip()
        .rstrip()
        .split("<|endofchunk|>")[0]
        .lstrip()
        .rstrip()
        .lstrip('"')
        .rstrip('"')
    )
    return parsed_output


if __name__ == "__main__":
    # parser = argparse.ArgumentParser()
    # parser.add_argument(
    #     "--model_path_or_name",
    #     type=str,
    #     default="luodian/otter-9b-hf",
    #     help="Path or name of the model (HF format)",
    # )
    # parser.add_argument(
    #     "--model_version_or_tag",
    #     type=str,
    #     default="apr25_otter",
    #     help="Version or tag of the model",
    # )
    # parser.add_argument(
    #     "--input_file",
    #     type=str,
    #     default="evaluation/sample_questions.json",
    #     help="Path of the input file",
    # )
    # args = parser.parse_args()
    model_path_or_name = "./aiart/checkpoint_5.pt"
    img_path = "/home/teddy/workspace/datasets/aiart/sdft/0070109.jpg"
    # prompt = "Describe this image."
    prompt = "Was the image generated by AI?"
    # prompt = "Give me prompts using StableDiffusion to generate this image."

    model = FlamingoForConditionalGeneration.from_pretrained(
        "luodian/openflamingo-9b-hf", device_map="auto"
    )
    tokenizer = model.text_tokenizer
    tokenizer.add_special_tokens(
        {"additional_special_tokens": ["<|endofchunk|>", "<image>", "<answer>"]}
    )
    model.lang_encoder.resize_token_embeddings(len(tokenizer))
    checkpoint = torch.load(model_path_or_name, map_location="cpu")
    model.load_state_dict(checkpoint["model_state_dict"], False)
    model.text_tokenizer.padding_side = "left"
    image_processor = transformers.CLIPImageProcessor()


    response = get_response(img_path, prompt)
    # import pdb;pdb.set_trace()
    print(response)