import requests
import torch
import transformers
import json
import os
from PIL import Image
# from otter.modeling_otter import OtterForConditionalGeneration
import argparse
from flamingo.modeling_flamingo import FlamingoForConditionalGeneration
import more_itertools
from tqdm import tqdm


def get_formatted_prompt(prompt: str) -> str:
    return f"<image> User: {prompt} GPT: <answer>"

def generate(modelpath, file_path, data_root, batch_size, output_file, prompt):
    model_path_or_name = modelpath
    model = FlamingoForConditionalGeneration.from_pretrained(
        "luodian/openflamingo-9b-hf", device_map="auto"
    )
    tokenizer = model.text_tokenizer
    tokenizer.add_special_tokens(
        {"additional_special_tokens": ["<|endofchunk|>", "<image>", "<answer>"]}
    )
    model.lang_encoder.resize_token_embeddings(len(tokenizer))
    if model_path_or_name != "luodian/openflamingo-9b-hf":
        checkpoint = torch.load(model_path_or_name, map_location="cpu")
        model.load_state_dict(checkpoint["model_state_dict"], False)
    model.text_tokenizer.padding_side = "left"
    image_processor = transformers.CLIPImageProcessor()

    test_set = []
    with open(file_path, 'r') as f:
        for line in f:
            parts = line.split()[0]
            test_set.append(parts)

    already_test = {}
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.split('\t')
                key = parts[0]
                values = parts[1].strip()
                already_test[key] = values

    f = open(output_file, 'w', encoding='utf-8')

    for batch in more_itertools.chunked(tqdm(test_set), batch_size):
        parsed_output = []
        ids = []
        batch_images = None
        for b in batch:
            if b not in already_test:
                image_path = os.path.join(data_root, b)
                query_image = Image.open(image_path)
                try:
                    image = image_processor.preprocess([query_image], return_tensors="pt")["pixel_values"].unsqueeze(1).unsqueeze(0)
                    if batch_images is None:
                        batch_images = image
                    else:
                        batch_images = torch.cat([batch_images, image], dim=0)
                    ids.append(b)
                except:
                    print(b)
            else:
                ids.append(b)
                parsed_output.append(already_test[b])
                print(b)

        if batch_images is not None:
            batch_text = [f"<image> User: {prompt} GPT: <answer>" for i in range(len(batch_images))]
            tokenizer.padding_side = "left"
            encodings = tokenizer(
                batch_text,
                padding="longest",
                truncation=True,
                return_tensors="pt",
                max_length=2000,
            )
            input_ids = encodings["input_ids"]
            attention_mask = encodings["attention_mask"]

            generated_text = model.generate(
                vision_x=batch_images.to(model.device),
                lang_x=input_ids.to(model.device),
                attention_mask=attention_mask.to(model.device),
                max_new_tokens=256,
                num_beams=3,
                no_repeat_ngram_size=3,
            )

            for item in generated_text:
                parsed_output.append(model.text_tokenizer.decode(item)
                    .split("<answer>")[1].lstrip().rstrip().split("<|endofchunk|>")[0].lstrip().rstrip().lstrip('"').rstrip(
                    '"'))

        for id, pred in zip(ids, parsed_output):
            line = "{}\t{}\n".format(id, pred)
            f.write(line)


if __name__ == "__main__":
    # modelpath = 'luodian/openflamingo-9b-hf'
    modelpath = '/home/teddy/workspace/otter/aiart3/checkpoint_5.pt'
    # file_path = "./prompts_1k.txt"
    file_path = "/home/teddy/workspace/datasets/aiart/deepfake_ood_test.txt"
    data_root = "/home/teddy/workspace/datasets/aiart/test/"
    batch_size = 10
    output_file = './our_ood.txt'
    # prompt = "Describe this image."
    prompt = "Is this image generated by AI?"
    # prompt = "Give me prompts using StableDiffusion to generate this image."
    generate(modelpath, file_path, data_root, batch_size, output_file, prompt)










